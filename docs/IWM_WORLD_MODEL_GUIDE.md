# IWM Vision World Model - Implementation Guide

**Status**: Phase 1 Complete ‚úÖ  
**Version**: 1.0  
**Date**: November 16, 2025

---

## Overview

The **IWM (Image World Model)** is a vision-based world model that learns latent representations of game/environment states and can predict future latents given actions. Based on JEPA/IWM research, it provides:

1. **Latent world modeling**: Compact 768-d representations of visual scenes
2. **Predictive capabilities**: Imagine future states given actions
3. **Dual abstraction streams** (Phase 2): Invariant (for LLM) + equivariant (for control)
4. **Action-conditioned planning** (Phase 3): k-step rollouts for trajectory planning

---

## Architecture

### Model Components

**Encoder** (ViT-B/16):
- Input: 224√ó224 RGB images
- Output: 768-d global latent (CLS token) + 196√ó768 patch latents
- Params: ~86M
- Pretrained on ImageNet + Skyrim mix

**Predictor** (Transformer):
- Input: Current latent + action/augmentation parameters
- Output: Predicted next latent
- Layers: 8 (Phase 1), 4 (invariant), 16 (equivariant)
- Params: ~60-150M depending on depth

**Total**: ~130-250M parameters (fits comfortably in 20GB VRAM)

---

## Phase 1: Core IWM (Current Implementation)

### What's Implemented

1. **Model Architecture** (`singularis/world_model/iwm_models.py`):
   - `IWM`: Complete model with encoder + predictor
   - `IWMConfig`: Configuration dataclass
   - `create_iwm_model()`: Factory for different variants

2. **FastAPI Service** (`singularis/world_model/iwm_service.py`):
   - `POST /encode`: Image ‚Üí latent
   - `POST /predict`: Latent + action ‚Üí predicted latent
   - `POST /rollout`: k-step latent rollouts
   - `GET /health`: Service status

3. **Client Library** (`singularis/world_model/iwm_client.py`):
   - `IWMClient`: Async client for calling service
   - Simple API: `encode_image()`, `predict_next()`, `rollout()`

4. **BeingState Integration** (`singularis/core/being_state.py`):
   - `vision_core_latent`: Current visual latent
   - `vision_prediction_surprise`: Prediction error metric
   - `vision_mrr`: World model confidence
   - Ready for Phase 2: `vision_semantic`, `vision_equivariant`

5. **Training Script** (`train_iwm_phase1.py`):
   - ViT-B/16 + 8-layer predictor
   - JEPA-style asymmetric augmentations
   - DDP support for dual 7900 XT
   - ~24-36 hours for 100 epochs

---

## Hardware Requirements

### Training (Phase 1)

**Single GPU (20GB)**:
- Batch size: 128-160
- Training time: 48-72 hours

**Dual GPU (2√ó20GB) with DDP**:
- Batch size: 256-320 (global)
- Training time: 24-36 hours ‚úÖ **Recommended**

### Inference

**Single GPU (20GB)**:
- Core IWM: ~8GB VRAM
- Dual-stream (Inv + Equi): ~16GB VRAM
- Plenty of headroom for other systems

---

## Quick Start

### 1. Install Dependencies

```bash
pip install torch torchvision fastapi uvicorn aiohttp pydantic loguru pillow
```

### 2. Train Model (Optional - or use pretrained)

```bash
# Prepare data directories
mkdir -p data/imagenet data/skyrim

# Single GPU
python train_iwm_phase1.py \
    --data_dirs data/imagenet data/skyrim \
    --device cuda:0 \
    --batch_size 160 \
    --epochs 100

# Dual GPU (DDP)
python -m torch.distributed.launch --nproc_per_node=2 train_iwm_phase1.py \
    --data_dirs data/imagenet data/skyrim \
    --batch_size 160 \
    --epochs 100 \
    --ddp
```

### 3. Start IWM Service

```bash
# Set environment variables
export IWM_MODEL_VARIANT=core
export IWM_MODEL_PATH=./checkpoints/iwm/iwm_core_final.pt
export IWM_DEVICE=cuda:0
export IWM_SERVICE_PORT=8001

# Start service
python -m singularis.world_model.iwm_service
```

Service will be available at `http://localhost:8001`

### 4. Use in Code

```python
from singularis.world_model import IWMClient
from singularis.core.being_state import BeingState
import numpy as np

# Initialize client
iwm = IWMClient("http://localhost:8001")

# Check health
health = await iwm.health()
print(health)

# Encode frame
frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
result = await iwm.encode_image(frame)

# Store in BeingState
being_state = BeingState()
being_state.vision_core_latent = result.z_cls
being_state.vision_core_timestamp = result.timestamp

# Predict next frame (with action)
pred = await iwm.predict_next(
    z_cls=result.z_cls,
    aug_params=[0.5, 0.3, 0.0]  # Action encoding
)

# Compute surprise
next_latent = await iwm.encode_image(next_frame)
surprise = np.linalg.norm(pred.z_cls_pred - next_latent.z_cls)
being_state.vision_prediction_surprise = surprise

# Rollout k steps for planning
rollout = await iwm.rollout(
    z_cls=result.z_cls,
    aug_seq=[[0.5, 0.3], [0.4, 0.4], [0.3, 0.5]]  # 3-step action sequence
)

# Use predicted latents for action selection
for i, z_pred in enumerate(rollout.z_cls_seq):
    print(f"Step {i}: MRR={rollout.mrr_seq[i]:.3f}, uncertainty={rollout.uncertainty_seq[i]:.3f}")
```

---

## Integration with ActionArbiter

### Perception Hook

Add to perception pipeline (e.g., in `singularis/perception/unified_perception.py`):

```python
from singularis.world_model import IWMClient

class UnifiedPerceptionLayer:
    def __init__(self, ...):
        # ... existing init ...
        self.iwm_client = IWMClient("http://localhost:8001")
    
    async def update_perception(self, frame: np.ndarray, being_state: BeingState):
        # ... existing perception ...
        
        # Encode to IWM latent
        try:
            result = await self.iwm_client.encode_image(frame)
            
            # Update BeingState
            being_state.vision_core_latent_prev = being_state.vision_core_latent
            being_state.vision_core_latent = result.z_cls
            being_state.vision_core_timestamp = result.timestamp
            being_state.vision_mrr = result.mrr if hasattr(result, 'mrr') else 0.0
            
            # Compute surprise if we have prev latent
            if being_state.vision_core_latent_prev is not None:
                # Simple prediction: assume identity (no action)
                pred = await self.iwm_client.predict_next(
                    being_state.vision_core_latent_prev,
                    aug_params=[0.0] * 16  # Neutral action
                )
                surprise = np.linalg.norm(pred.z_cls_pred - result.z_cls)
                being_state.vision_prediction_surprise = surprise
            
        except Exception as e:
            logger.warning(f"IWM encoding failed: {e}")
```

### ActionArbiter Hook

Add to `singularis/skyrim/action_arbiter.py`:

```python
async def request_action(self, ...):
    # ... existing logic ...
    
    # Check vision surprise (high surprise = uncertain state)
    if being_state.vision_prediction_surprise > 2.0:
        logger.warning(
            f"High vision surprise: {being_state.vision_prediction_surprise:.2f}, "
            "slowing down for observation"
        )
        # Maybe add a delay or prefer "wait" action
    
    # Use MRR as confidence signal
    if being_state.vision_mrr < 0.5:
        logger.info(f"Low world model confidence (MRR={being_state.vision_mrr:.2f})")
        # Maybe escalate to LLM
```

---

## Phase 2: Dual-Stream (Future)

### Plan

1. **Train two variants**:
   - `IWMInv` (invariant): 4-layer predictor, more abstract
   - `IWMEqui` (equivariant): 16-layer predictor, preserves structure

2. **Run both in service**:
   - Load both models
   - Add `/encode_dual` endpoint returning both latents

3. **Wire to BeingState**:
   - `vision_semantic` ‚Üê IWMInv (for LLM reasoning)
   - `vision_equivariant` ‚Üê IWMEqui (for control/RL)

4. **Use in system**:
   - LLM orchestrator reads `vision_semantic`
   - Reflex controller reads `vision_equivariant`

---

## Phase 3: Action-Conditioned (Future)

### Plan

1. **Collect gameplay data**:
   - Log `(frame_t, action, frame_t+1)` tuples during Skyrim play

2. **Extend augmentation params**:
   - Add discrete game action tokens (move, attack, turn, etc.)
   - Train predictor conditioned on real actions

3. **Planning with rollouts**:
   - ActionArbiter generates candidate action sequences
   - Calls `/rollout` with each sequence
   - Scores trajectories by predicted MRR/surprise
   - Picks best sequence

---

## Configuration

### Environment Variables

```bash
# Service
IWM_MODEL_VARIANT=core  # or 'invariant', 'equivariant'
IWM_MODEL_PATH=./checkpoints/iwm/iwm_core_final.pt
IWM_DEVICE=cuda:0  # or cuda:1, cpu
IWM_SERVICE_HOST=0.0.0.0
IWM_SERVICE_PORT=8001

# Training
IWM_DATA_DIRS=./data/imagenet,./data/skyrim
IWM_BATCH_SIZE=160
IWM_EPOCHS=100
IWM_LR=4e-4
```

### Model Variants

| Variant | Predictor Depth | Use Case | VRAM (Inf) |
|---------|----------------|----------|------------|
| `core` | 8 layers | General world model | ~8GB |
| `invariant` | 4 layers | Semantic/LLM reasoning | ~6GB |
| `equivariant` | 16 layers | Control/physics | ~12GB |

---

## Metrics & Monitoring

### World Model Metrics

1. **MRR (Mean Reciprocal Rank)**:
   - Measures how well predictor ranks correct next latent
   - Higher = more confident world model
   - Typical: 0.6-0.9

2. **Prediction Surprise**:
   - `||predicted - actual||` in latent space
   - Higher = more unexpected transition
   - Use as anomaly detection / uncertainty

3. **Abstraction Index**:
   - Where on invariant ‚Üî equivariant spectrum
   - Phase 2 feature

### Integration with Temporal Binding

Surprise feeds into temporal coherence:

```python
# In temporal tracker
if being_state.vision_prediction_surprise > 3.0:
    # High surprise = potential stuck loop or novel situation
    temporal_coherence -= 0.1
```

---

## Troubleshooting

### OOM during training

- Reduce batch size: `--batch_size 128`
- Enable gradient checkpointing (modify `train_iwm_phase1.py`)
- Use mixed precision (already enabled with `autocast`)

### Service not responding

- Check service logs
- Verify model checkpoint exists
- Test with `/health` endpoint

### Poor prediction quality

- Train longer (200 epochs)
- Check data quality (corrupted images?)
- Increase predictor depth
- Verify EMA is updating

---

## Performance Benchmarks

### Training (Dual 7900 XT, Batch 320)

- Epoch time: ~15-20 minutes
- 100 epochs: ~25-30 hours
- Final loss: ~0.3-0.4 (cosine similarity)

### Inference (Single 7900 XT)

- `/encode`: ~10-15ms per image
- `/predict`: ~5-8ms
- `/rollout` (k=5): ~30-40ms

Bottleneck is usually API overhead, not model inference.

---

## Next Steps

1. ‚úÖ **Phase 1 Complete**: Core IWM trained and deployed
2. üîÑ **Perception Integration**: Wire into unified perception layer
3. üîÑ **Temporal Metrics**: Use surprise in temporal binding
4. ‚è≥ **Phase 2**: Train dual-stream (invariant + equivariant)
5. ‚è≥ **Phase 3**: Collect gameplay data, train action-conditioned IWM
6. ‚è≥ **ActionArbiter Planning**: Use rollouts for trajectory planning

---

## References

- **JEPA Paper**: LeCun et al., "A Path Towards Autonomous Machine Intelligence"
- **I-JEPA**: Assran et al., "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"
- **Vision Transformers**: Dosovitskiy et al., "An Image is Worth 16x16 Words"

---

**Questions?** See `singularis/world_model/` for implementation details.
